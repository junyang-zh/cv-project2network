import pylayer as L

'''
    utility: pointwise add two "2-d" lists with weight
    a = a * wa + b * wb
'''
def list2d_add(a, wa, b, wb):
    for x, y in zip(a, b):
        for u, v in zip(x, y):
            u *= wa
            u += v * wb

class SGD(object):
    '''
        Implements stochastic gradient descent, with momentum features.

        The model could be sequential, must have forward(), backward();
        model.param_grads is generated by model.backward().
    '''
    def __init__(self, model, lr=1e-3, momentum=0.):
        self.lr = lr
        self.momentum = momentum
        self.last_step_grads = None
        self.model = model
        # maintains a reference of all trainable params of all layers
        self.params_ref = []
        for l in self.model.layers:
            # crappy implementation, not my bad
            if (isinstance(l, L.Linear)):
                self.params_ref.append((l.weight, l.bias))
            elif (isinstance(l, L.BatchNorm1d)):
                self.params_ref.append((l.gamma, l.beta))
            elif (isinstance(l, L.ReLU)):
                self.params_ref.append(())
            elif (isinstance(l, L.CrossEntropyLossWithSoftmax)):
                self.params_ref.append(())
            elif (isinstance(l, L.Conv2d)):
                self.params_ref.append((l.weight, l.bias))
            elif (isinstance(l, L.MaxPool2d)):
                self.params_ref.append(())


    def step(self):
        if (self.momentum > 0.):
            list2d_add( self.model.param_grads, 1. - self.momentum,
                        self.last_step_grads, self.momentum)
        list2d_add(self.params_ref, 1., self.model.param_grads, self.lr)
        self.last_step_grads = self.model.param_grads
